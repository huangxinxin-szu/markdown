[TOC]

### 环境信息

master: power32,power22,power23

dns服务器: power21

haproxy: power21

worker: power22,power23,power27,power28

### 环境准备


#### 安装DNS服务

```shell

# 配置DNS
yum install -y dnsmasq
# 在/etc/dnsmasq.conf中添加以下配置:
ddn-hosts=/etc/dnsmasq.hosts
listen-address=127.0.0.1,192.168.1.123 # 192.168.1.123为主机IP

# dns服务器: 10.222.111.5
# 添加要解析的域名
vim /etc/dnsmasq.hosts

# 重启服务
systemctl restart dnsmasq
systemctl enable dnsmasq
```

#### 安装Haproxy

```shell
yum install haproxy
# 在/etc/haproxy/haproxy.cfg中加入以下配置:
frontend k8s_apiservers
    bind 0.0.0.0:80
    mode tcp
    log global
    use_backend apiservers

backend apiservers
    mode tcp
    balance source
    server apiserver1 k8s-01:6443 check inter 2000 rise 3 fall 3 weight 2

# 重启服务
systemctl enable haproxy
systemctl start haproxy
```



#### Linux环境初始化

##### 关闭swap

```shell
# 关闭swap（已关闭）
```

##### 配置DNS

```shell
cat > /etc/resolv.conf <<EOF
nameserver 10.222.111.5
EOF
```




##### 安装Docker
已安装，版本18.09.3，升级到19.03

```shell
yum-config-manager     --add-repo     https://download.docker.com/linux/centos/docker-ce.repo
yum install docker-ce docker-ce-cli containerd.io --downloadonly --downloaddir=.
# 把rpm包拷到机器上
yum localupdate *.rpm
```



修改cgroup driver为systemd：

```shell
mkdir /etc/docker
cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ],
  "insecure-registries": [
       "https://89.72.10.30"
  ]
}
EOF
mkdir -p /etc/systemd/system/docker.service.d
systemctl daemon-reload
systemctl restart docker
systemctl enable docker

# 验证配置是否成功
[root@power21 v1.16.2]# docker info | grep -i cgroup
Cgroup Driver: systemd


```

##### 清除原K8S环境资源

```shell
kubeadm reset
systemctl stop etcd
systemctl disable etcd
rm -rf /var/lib/etcd # 删除etcd数据
```



### 安装etcd

```shell
# https://www.cnblogs.com/zll-0405/p/10786570.html
# clean
mv /etc/etcd/etcd.conf /etc/etcd/etcd.conf.bak
rm -rf /var/lib/etcd/*
mv /usr/local/bin/etcd /usr/local/bin/etcd.bak
mv /usr/local/bin/etcdctl  /usr/local/bin/etcdctl.bak
mv /usr/lib/systemd/system/etcd.service /tmp/etcd.service.bak

MODULEPATH=/usr/share/Modules/modulefiles:/etc/modulefiles:/public/fgn/modulefiles
module load proxychains


export HOST0=10.222.111.9
export HOST1=10.222.111.6 # power22
export HOST2=10.222.111.7 # power23

# 先生成证书

tar zxvf etcd-v3.4.3-linux-amd64.tar.gz
cd etcd-v3.4.3-linux-amd64
cp etcd etcdctl /usr/local/bin/
mkdir -p /var/lib/etcd

# master












# docker
mkdir /etc/systemd/system/kubelet.service.d/
cat << EOF > /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
[Service]
ExecStart=
#  Replace "systemd" with the cgroup driver of your container runtime. The default value in the kubelet is "cgroupfs".
ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --cgroup-driver=systemd
Restart=always
EOF

systemctl daemon-reload
systemctl restart kubelet


# Update HOST0, HOST1, and HOST2 with the IPs or resolvable names of your hosts

# Create temp directories to store files that will end up on other hosts.
mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/
ETCDHOSTS=(${HOST0} ${HOST1} ${HOST2})
NAMES=("infra0" "infra1" "infra2")
for i in "${!ETCDHOSTS[@]}"; do   # 0 1 2 
HOST=${ETCDHOSTS[$i]}
NAME=${NAMES[$i]}
cat << EOF > /tmp/${HOST}/kubeadmcfg.yaml
apiVersion: "kubeadm.k8s.io/v1beta2"
kind: ClusterConfiguration
etcd:
    local:
        serverCertSANs:
        - "${HOST}"
        peerCertSANs:
        - "${HOST}"
        extraArgs:
            initial-cluster: ${NAMES[0]}=https://${ETCDHOSTS[0]}:2380,${NAMES[1]}=https://${ETCDHOSTS[1]}:2380,${NAMES[2]}=https://${ETCDHOSTS[2]}:2380
            initial-cluster-state: new
            name: ${NAME}
            listen-peer-urls: https://${HOST}:2380
            listen-client-urls: https://${HOST}:2379
            advertise-client-urls: https://${HOST}:2379
            initial-advertise-peer-urls: https://${HOST}:2380
EOF
done

# in host0
kubeadm init phase certs etcd-ca # /etc/kubernetes/pki/etcd/
kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST2}/
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml
cp -R /etc/kubernetes/pki /tmp/${HOST1}/
find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete

kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml
kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml

find /tmp/${HOST2} -name ca.key -type f -delete
find /tmp/${HOST1} -name ca.key -type f -delete

# 将/host*/pki目录下的文件考到host1 和 host2的/root/etcd/pki/目录下，把kubeadmcfg.yaml也拷到相应机器上

# 在所有机器上创建etcd static pod
kubeadm init phase etcd local --config=/tmp/${HOST0}/kubeadmcfg.yaml

```



### k8s  master节点安装

安装kubelet kubeadm kubectl ：

```shell
yum remove kubelet kubeadm kubectl
yum install kubelet kubeadm kubectl # 版本 v1.16.2
systemctl enable --now kubelet

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables=1
net.bridge.bridge-nf-call-iptables=1
EOF
sysctl --system

```

配置cgroup:

```shell
docker info | grep -i cgroup 

# 如果cgroup不是cgroupfs的话(比如systemd)，则需要在/etc/sysconfig/kubelet中添加一下内容
KUBELET_EXTRA_ARGS=--cgroup-driver=systemd

# then restart it
systemctl daemon-reload
systemctl restart kubelet
```

手动导入依赖镜像：

```shell
[root@power21 ~]# kubeadm config images list
k8s.gcr.io/kube-apiserver:v1.16.2
k8s.gcr.io/kube-controller-manager:v1.16.2
k8s.gcr.io/kube-scheduler:v1.16.2
k8s.gcr.io/kube-proxy:v1.16.2
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd:3.3.15-0
k8s.gcr.io/coredns:1.6.2

docker load < kube-proxy.tar
docker load < kube-apiserver.tar
docker load < kube-controller-manager.tar
docker load < kube-scheduler.tar
docker load < pause.tar
docker load < etcd.tar
docker load < coredns.tar
```

初始化:

```shell
# 第一个master
kubeadm init --apiserver-advertise-address=10.222.111.16 --control-plane-endpoint "k8s-proxy1:80" --pod-network-cidr=192.168.0.0/16 --upload-certs

# 2-n个master
kubeadm join k8s-proxy1:80 --token zl265s.t5vak501y9lp3n3k \
    --discovery-token-ca-cert-hash sha256:0ec1322882ebb9853c4ac066de319e66e1349e2684007e70006ea987a25a2d53 \
    --control-plane --certificate-key 001cdfc839975b9d040e7d54d76f0a919ecb375edfa05c184e84ef9d00fd0894

# worker
kubeadm join k8s-proxy1:80 --token zl265s.t5vak501y9lp3n3k \
    --discovery-token-ca-cert-hash sha256:0ec1322882ebb9853c4ac066de319e66e1349e2684007e70006ea987a25a2d53

# 配置kubectl
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

    
# 验证安装是否成功
kubectl version
```

安装calico网络：

```shell
docker load < cni.tar
docker load < node.tar
docker load < pod2daemon-flexvol.tar
docker load < kube-controllers.tar

kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml
```



允许Pod创建在该控制面板节点上：

```shell
kubectl taint node  node-role.kubernetes.io/master-
#添加污点 kubectl taint nodes master1 node-role.kubernetes.io/master=:NoSchedule
```

### 安装脚本

```shell
# turn off swap
swapoff -a 
sed -i '/ swap / s/^/#/' /etc/fstab 

timedatectl set-timezone Asia/Shanghai

mkdir /public
mount 89.72.10.31:/data/ssd/nfs_share /public
MODULEPATH=$MODULEPATH:/public/fgn/modulefiles
module load proxychains

# update docker's version to 19.03
cd /public/hxx/k8s/v1.16.2/docker19.03/
proxychains4 yum localupdate *.rpm -y

cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ],
  "insecure-registries": [
       "https://89.72.10.30"
  ]
}
EOF
mkdir -p /etc/systemd/system/docker.service.d
systemctl daemon-reload
systemctl restart docker
systemctl enable docker

kubeadm reset
systemctl stop etcd
systemctl disable etcd
rm -rf /var/lib/etcd # 删除etcd数据


cat <<EOF > /etc/yum.repos.d/kubenetes_ali.repo
[kubernetes]
name=Kubernetes Repo
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
enabled=1
EOF

# MODULEPATH=$MODULEPATH:/public/fgn/modulefiles
proxychains4 yum remove kubelet kubeadm kubectl -y
proxychains4 yum install kubelet kubeadm kubectl -y

systemctl enable --now kubelet

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system

cat > /etc/sysconfig/kubelet <<EOF
KUBELET_EXTRA_ARGS=--cgroup-driver=systemd
EOF
systemctl daemon-reload
systemctl restart kubelet

cd /public/hxx/k8s/v1.16.2/
docker load < kube-proxy.tar
docker load < kube-apiserver.tar
docker load < kube-controller-manager.tar
docker load < kube-scheduler.tar
docker load < pause.tar
docker load < etcd.tar
docker load < coredns.tar

cd /public/hxx/k8s/v1.16.2/calico/
docker load < cni.tar
docker load < node.tar
docker load < pod2daemon-flexvol.tar
docker load < kube-controllers.tar

# config dns server
cp /etc/resolv.conf /etc/resolv.conf.bak
cat > /etc/resolv.conf <<EOF
nameserver 10.222.111.5
EOF

# 第2-n个master
kubeadm join k8s-proxy1:80 --token 3gph90.ndhuut2h0z8d6vtb \
    --discovery-token-ca-cert-hash sha256:0ec1322882ebb9853c4ac066de319e66e1349e2684007e70006ea987a25a2d53 \
    --control-plane --certificate-key df2a2338a47f4cdf7ca7f3803ce7a9b642e3f9287edc14bde7647f941ea07b39

# kubeadm token create
 #kubeadm init phase upload-certs --upload-certs
    
# worker
kubeadm join k8s-proxy1:80 --token zl265s.t5vak501y9lp3n3k \
    --discovery-token-ca-cert-hash sha256:0ec1322882ebb9853c4ac066de319e66e1349e2684007e70006ea987a25a2d53


# 第一个master
kubectl apply -f calico.yaml

# 第2,3个master
kubectl taint nodes --all node-role.kubernetes.io/master-

```



