[TOC]



## Abstract

本文档描述了消息传递接口(Message-Passing Interface, MPI)标准，3.1版本。



## 1 Introduction to MPI 

### 1.1 Overview and Goals 

MPI(Message-Passing Interface )是一个消息传递库接口规范。这个定义的所有部分都很重要。MPI主要处理消息传递并行编程模型，其中数据通过每个进程的协作操作从一个进程的地址空间移动到另一个进程的地址空间。MPI是一种规范，而不是实现;MPI有多种实现。此规范用于库接口;MPI不是一种语言，所有的MPI操作都表示为函数、子程序或方法，根据C和Fortran语言是MPI标准的一部分。该标准是由一个由并行计算供应商、计算机科学家和应用程序开发人员组成的社区通过一个开放的过程定义的。接下来的几节将概述MPI的开发历史。

建立消息传递标准的主要优点是可移植性和易用性。在分布式内存通信环境中，更高级别的例程和/或抽象构建在更低级别的消息传递例程之上，标准化的好处尤其明显。此外，本文所建议的消息处理标准的定义为供应商提供了一组清晰定义的基本例程，它们可以有效地实现这些例程，或者在某些情况下，它们可以提供硬件支持，从而增强可伸缩性。

简单地说，消息传递接口的目标是为编写消息传递程序开发一个广泛使用的标准。因此，接口应该为消息传递建立一个实用、可移植、高效和灵活的标准。

下面是一个完整的目标列表。

- 设计应用程序编程接口(不一定适用于编译器或系统实现库)
- 允许有效的通信:避免内存到内存的复制，允许计算和通信重叠，并在可能的情况下将负载转移给通信协处理器
- 允许在异构环境中使用的实现
- 为接口提供方便的C和Fortran绑定
- 假设有一个可靠的通信接口:用户不需要处理通信失败。这些故障由底层通信子系统处理
- 定义一个可以在许多供应商平台上实现的接口，而不需要对底层通信和系统软件进行重大更改
- 接口的语义应该是独立于语言的
- 接口的设计应考虑线程安全

### 1.2 Background of MPI-1.0 

MPI试图利用许多现有消息处理系统的最吸引人的特性，而不是选择其中一个作为标准。因此,MPI深受IBM T. J. Watson研究中心等工作的影响。

MPI标准化工作涉及来自40个组织的约60人，主要来自美国和欧洲。大多数并发计算机的主要供应商都参与了MPI，还有来自大学、政府实验室和工业界的研究人员。标准化过程始于分布式内存环境中消息处理标准研讨会，该研讨会由并行计算研究中心主办，1992年4月29-30日，在弗吉尼亚州威廉斯堡举行。在这个研讨会上，讨论了标准消息传递接口的基本特性，并建立了一个工作组来继续标准化过程。

一个初步的提案草案，称为MPI-1，是由Dongarra, Hempel,Hey, 和Walker在1992年11月发布，修订版在1993年2月完成。MPI-1体现了威廉斯堡研讨会确定的在消息传递标准中必需的主要特性。由于MPI-1的主要目的是促进讨论并开始工作，它主要侧重于点对点通信。MPI-1带来了许多重要的标准化问题，但它不包含任何集体通信例程，而且不具有线程安全性。

1992年11月，MPI工作组在明尼阿波利斯举行了一次会议，会上决定将标准化进程置于更正式的基础之上，并普遍采用高性能Fortran论坛的程序和组织。为标准的主要组成部分领域成立了小组委员会，并为每个小组委员会设立了电子邮件讨论服务。此外，还确定了在1993年秋季以前拟订MPI标准草案的目标。为了实现这一目标，MPI工作组在1993年头9个月里每6周进行一次为期2天的会议，并在1993年11月的超级计算机93会议上提出了MPI标准草案。这些会议和电子邮件讨论共同组成了MPI论坛，其成员已向高性能计算社区的所有成员开放。

### 1.3 Background of MPI-1.1, MPI-1.2, and MPI-2.0 

从1995年3月开始，MPI论坛开始讨论对原来的MPI标准文档的修改和扩展。**这些讨论的第一个成果是MPI 1.1规范，发布于1995年6月**。当时的努力集中在五个方面：

1. 对MPI-1.1文件的进一步更正和澄清
2. 添加到MPI-1.1中，不会显著改变其功能类型(新的数据类型构造函数、语言互操作性等)
3. 全新的功能类型(动态流程、单向通信、并行I/O等)，这是人们认为的MPI-2的功能
4. Bindings for Fortran 90 and C++. MPI-2 specifies C++ bindings for both MPI-1 and MPI-2 functions, and extensions to the Fortran 77 binding of MPI-1 and MPI-2 to handle Fortran 90 issues 
5. 讨论MPI过程和框架可能有用的领域，但是在标准化之前需要更多的讨论和经验(例如，共享内存机器上的零拷贝语义、实时规范)

这种结构使用户和实现者很容易理解一个给定的实现具有MPI遵从性：

- 符合MPI-1意味着符合MPI-1.3。这是一个有用的遵从性级别。这意味着实现符合MPI-2文档第3章中对MPI-1.1函数行为的说明。有些实现可能要求更改符合MPI-1。
- 符合MPI-2意味着符合所有MPI-2.1

需要强调的是，保持了正向兼容性。也就是说，一个有效的MPI-1.1程序既是一个有效的MPI-1.3程序，也是一个有效的MPI-2.1程序，一个有效的MPI-1.3程序也是一个有效的MPI-2.1程序。（向下兼容）



### 1.4 Background of MPI-1.3 and MPI-2.1 

在发布了MPI-2.0之后，MPI论坛继续对两个标准文档(MPI-1.1和MPI-2.0)进行勘误表和澄清。MPI-1.1的简短文档“勘误表”于1998年10月12日发布。2001年7月5日，进行了第一次勘误和澄清投票发布了MPI-2.0，并于2002年5月22日进行了第二次投票。两个投票都是电子投票。这两个投票在2002年5月15日被合并成一个文档《MPI-2勘误表》。然后中断了这个勘误表过程，但该论坛及其电子邮件反射器仍在处理要求澄清的新请求。

三次会议（EuroPVM/MPI’06 in Bonn, at EuroPVM/MPI’07 in Paris, and at SC’07 in Reno ）重启MPI论坛正常工作。2007年12月，一个指导委员会开始组织新的MPI论坛会议，每隔8周举行一次。在2008年1月14日-16日在芝加哥举行的会议上，MPI论坛决定将现有的和未来的MPI文档合并为每个MPI标准版本的一个文档。由于技术和历史原因，本系列始于MPI 1.3。另外的投票3和4解决了从1995年开始的勘误表中的旧问题到最近几年的新问题。所有文件(MPI-1.1, MPI-2，MPI-1.1勘误表(1998年10月12日)和MPI-2.1选票1-4) 合并为一份文件草案，每个章节都定义了一个章节作者和评审团队。他们清理了文档以实现一致的MPI-2.1文档。最终的MPI-2.1标准文档于2008年6月完成，最终在2008年9月都柏林会议上通过了第二次投票。当前MPI的主要工作论坛是MPI-3的准备工作。



### 1.5 Background of MPI-2.2 

MPI-2.2是对MPI-2.1标准的一个小更新。这个版本解决了MPI-2.1标准中没有纠正的额外错误和歧义，以及满足以下标准的少量对MPI-2.1的扩展：

- 任何正确的MPI-2.1程序都是正确的MPI-2.2程序
- 任何扩展都必须为用户带来显著的好处
- 任何扩展都不需要大量的实现工作。为此，所有这些更改都伴随着一个开源实现

MPI-2.2的讨论与MPI-3的讨论同时进行;在某些情况下，建议对MPI-2.2进行扩展，但后来迁移到了MPI-3。

### 1.6 Background of MPI-3.0 

MPI-3.0是对MPI标准的一个主要更新。

### 1.7 Background of MPI-3.1 

MPI-3.1是对MPI标准的一个小更新。大多数更新都是对标准的修正和澄清，特别是对于Fortran绑定。



### 1.9 What Platforms Are Targets For Implementation? 

消息传递范式的吸引力至少部分源于其广泛的可移植性。结合共享和分布式内存视图的体系结构，或者网络速度的提高，都不会使范式过时。因此，在各种各样的机器上实现这个标准应该是可能的，也是有用的，包括那些“由其他机器的集合组成的\机器”，并行的或不并行的，由通信网络连接的。

MPI提供了许多特性，旨在提高具有专用处理器间通信硬件的可伸缩并行计算机的性能。因此，我们期望在这样的机器上提供本机的、高性能的MPI实现。同时，基于标准Unix处理器间通信协议的MPI实现将为工作站集群和工作站的异构网络提供可移植性。



### 1.11 What Is Not Included In The Standard? 

已经考虑了很多特性，但是没有包含在这个标准中。出现这种情况的原因有很多，其中之一是在完成标准时强加的时间限制。不包含的特性总是可以由特定的实现作为扩展提供。也许MPI的未来版本将解决其中的一些问题。



## 2 MPI Terms and Conventions 

本章解释了MPI文档中使用的标注术语和约定，已经做出的一些选择，以及这些选择背后的基本原理。

### 2.1 文档符号

#### Rationale 

在本文档中，接口规范中设计选择的基本原理以这种格式设置。有些读者可能希望跳过这些部分，而对接口设计感兴趣的读者可能希望仔细阅读它们。

#### Advice to users 

在整个文档中，以用户为目标的材料和说明用法的材料都以这种格式设置。有些读者可能希望跳过这些部分，而对MPI编程感兴趣的读者可能希望仔细阅读它们

#### Advice to implementors 

在整个文档中，主要是对实现者进行注释的材料都以这种格式设置。有些读者可能希望跳过这些部分，而对MPI实现感兴趣的读者可能希望仔细阅读它们。



### 2.2 命名规范

在许多情况下，C函数的MPI名称的形式是MPI_Class_action_subset ，这个约定起源于MPI-1。自MPI-2以来，已经尝试根据以下规则对MPI函数的名称进行标准化：

1. 在C语言中，所有与特定类型的MPI对象相关联的例程都应该是MPI_Class_action_subset形式的，如果不存在子集，则应该是MPI_Class_action形式的
2. 如果例程不与类关联，则其名称在C中应该是这种形式的：MPI_Action_subset 
3. 某些操作的名称已经标准化。具体来说，Create创建一个新对象，Get检索一个对象的信息，set设置这个信息，
   Delete删除信息，Is询问对象是否具有某个属性

MPI标识符被限制为30个字符(带有概要分析接口的31个字符)。这样做是为了避免超出某些编译系统的限制.



### 2.3 Procedure Specification 

MPI过程使用与语言无关的符号指定。MPI的定义尽量避免使用INOUT参数，因为这种使用很容易出错，尤其是对于标量参数。过程调用的参数被标记为IN、OUT或INOUT。它们的含义是:

- IN: 调用可以使用输入值，但不会在调用执行期间的任何时候从调用方的角度更新参数
- OUT : 调用可以更新参数，但不使用其输入值
- INOUT: 调用可以使用和更新参数

有一个特例,如果参数是opaque object的句柄,对象由过程调用更新，然后参数被标记为INOUT或OUT。即使句柄本身没有修改，它也是这样标记的。我们使用INOUT或OUT属性来表示更新了句柄引用的内容。



#### 2.5.4 Named Constants 

MPI过程有时为基本类型参数的特殊值赋予特殊意义。例如，tag是点对点通信操作的一个整数值参数,有特殊的通配符值:MPI_ANY_TAG 。这样的参数将有一个常规范围，它是对应的基本类型的值范围的一个适当的子范围。特殊值(如MPI_ANY_TAG)将位于常规范围之外。常规值的范围，如tag，可以使用环境查询功能查询，见第8章。其他值(如source)的范围取决于其他MPI例程给出的值。

MPI还提供了预定义的命名常量句柄，比如MPI_COMM_WORLD。除了下面提到的Fortran的例外，所有命名的常量都可以用于初始化表达式或赋值，但不一定用于数组声明或作为C switch或Fortran select/case语句中的标签。这意味着命名常量是链接时的常量，但不一定是编译时的常量。下面列出的命名常量必须是C和Fortran语言中的编译时常量。这些常量不会在执行期间更改值。定义了通过常量句柄访问的不透明对象，并且在MPI初始化(MPI_INIT)和MPI完成之间不改变值
(MPI_FINALIZE)。需要作为编译时常量(因此可以用于C switch和Fortran case/select语句中的数组长度声明和标签)的常量有:

MPI_MAX_PROCESSOR_NAME
MPI_MAX_LIBRARY_VERSION_STRING
MPI_MAX_ERROR_STRING
MPI_MAX_DATAREP_STRING
MPI_MAX_INFO_KEY
MPI_MAX_INFO_VAL
MPI_MAX_OBJECT_NAME
MPI_MAX_PORT_NAME
MPI_VERSION
MPI_SUBVERSION
MPI_STATUS_SIZE (Fortran only)
MPI_ADDRESS_KIND (Fortran only)
MPI_COUNT_KIND (Fortran only)
MPI_INTEGER_KIND (Fortran only)
MPI_OFFSET_KIND (Fortran only)
MPI_SUBARRAYS_SUPPORTED (Fortran only)
MPI_ASYNC_PROTECTS_NONBLOCKING (Fortran only) 

#### 2.5.5 Choice 

MPI函数有时使用带有choice(或choice 数据类型的参数。对同一例程的不同调用可以通过引用不同类型的实际参数来传递。提供这些参数的机制因语言而异。



#### 2.5.6 Absolute Addresses and Relative Address Displacements 

一些MPI过程使用address 参数来表示调用程序中的absolute address，或者使用relative displacement 参数来表示两个绝对地址之间的差异。这类参数的数据类型是C中的MPI_Aint。

